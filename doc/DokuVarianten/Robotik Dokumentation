1 Einleitung	2
2 Zielsetzung	2
3 Grundlagen	2
3.1 Hardwareplattform	3
3.2 Softwareplattform	3
4 Implementierung	4
4.1 Umgebung	4
4.2 Struktur	4
4.2.1 Linienverfolgung	4
4.2.2 Abstandserkennung	5
4.2.3 Motoransteuerung	6
4.2.4 Interprozesskommunikation	7
4.3 Komplikation und Fehler	7
5 Auswertung	7
6 Literaturverzeichnis	8

1 Einleitung

Autos sollen in der heutigen Welt immer intelligenter werden. Dies ist auch schon über viele Assistenzsysteme geschehen, 
doch die Entwicklung soll weiter gehen. Das Auto, das selbst fahren kann. Viele Unternehmen treiben diese Forschung voran, 
darunter auch Audi.
Im Rahmen des  „Audi Autonomous Driving Cup“ lässt Audi Teams gegeneinander antreten. Die Disziplinen sind eine Parkourfahrt 
und eine „Open Challange“. Bewertet wird die Performance der einzelnen Fahrzeuge.
Im Robotikpraktikum der FH-Wedel, haben wir die Möglichkeit uns mit dem 2017er Modell des Audi Cups näher auseinander zu setzten 
und erste Schritte zu tätigen, das Modellauto selbstständig fahren zu lassen.
Unsere Aufgabe bestand darin ein eigenes Ziel zu definieren und dieses zu verwirklichen.

2 Zielsetzung

Ziel ist es, das Modellauto in die Lage zu versetzen, einer Fahrbahnmarkierung selbstständig fol-gen zu können, ohne dabei 
mit Hindernissen zu kollidieren. Dafür wird auf die eingehenden Daten der v RGB-Frontkamera sowie der Ultraschallsensoren 
zugegriffen.
Die Fahrbahn wird mittig und durchgehend mit blauem Klebeband markiert. Die Lichtverhältnisse  müssen nicht konstant sein; 
die Auswertung der Kamerabilder soll aber jederzeit möglich sein. Hindernisse sind derart beschaffen, dass sie von den 
Ultraschallsensoren detektiert werden können (Mindestgröße, Schallharte Oberfläche, usw. sind gewährleistet).
Ist ein Hindernis zu nah an bzw. auf der Fahrbahn, soll das Modellauto langsamer werden und vor dem Hindernis anhalten. 
Das Modellauto fährt erst weiter, wenn die Ultraschallsensoren signalisie-ren, dass die Fahrbahn wieder frei ist.

3 Grundlagen

Im Folgenden werden die Grundlagen, bestehend aus Aufbau des genutzten Modellautos, der da-rauf laufenden Software, sowie das 
genutzte Framework erläutert, welche für das Verständnis des Projektes essenziell sind.

3.1 Hardwareplattform
Hardwareplattform

Das Audimodell bietet verschiedene Hardwarekomponenten, um das autonome Fahren zu verwirklichen.

TODO: Bild vom Audi gesamt

Herzstück des Audi ist das „GIGABYTE GA-Z170N-WIFI miniITX“ Mainboard. Unterstützt wird es von einer „NVIDIA GeForce GTX 1050Ti“. 

TODO: Bild vom Audi, vor allem mit der Intel 

Die Linienerkennung arbeitet mit der Intelkamera, die mit einer anderen Position und einem anderen Winkel benutzt wird, als im 
Basismodell. Zum Vergleich siehe Abbild 1 und 2. Grund ist der verbesserte Blick auf den Nahbereich und das mittig ausgerichtete Bild.

TODO: Bild von den Sensoren und Arduinos

Zur Kollisionsvermeidung werden die verbauten Ultrasonic Sensoren verwendet. Ansteuern lassen sich diese über Arduinos, die mit 
dem Mainboard verbunden sind. Der Audi besitzt insgesamt zehn Stück, von denen fünf, in unterschiedlichen Winkeln, nach vorne, 
jeweils einer zu jeder Seite und drei, wieder in unterschiedlichen Winkeln, nach hinten ausgerichtet sind. Die drei hinteren 
Sensoren bekommen nicht so viel Gewichtung, da das Auto in unserem Projekt nicht rückwärtsfahren soll.

Angetrieben wird das Modell von vier „Hacker SKALAR 10 21.5 Brushless Motor 1/10“.
Die Lenkung funktioniert über „Absima "ACS1615SG" Combat Series“ Servos. 

Die Stromversorgung während der Fahrt werden durch zwei Akkus sichergestellt.


3.2 Softwareplattform
Das Modellauto verwendet die Ubuntu Version 16.04 und zur Bildschirmübertragung ist der x11vcn Serverdienst eingerichtet. Die 
Software EB Assist ADTF 2.14 von Elektrobit ist installiert, um die damit bereitgestellte Programmoberfläche und das Automotive 
Data and Time-Triggered Framework zu verwenden. Das Framework stellt Funktionen zur Kommunikation mit den im Abschnitt 
„Hardwareplattform“ erläuterten Sensoren und Kameras bereit. Außerdem ermöglicht es eine Programmaufteilung in Funktionsbloecken,
welche parallel auf mehreren Prozessorkernen ausgeführt werden. Die Kommunikation zwischen diesen Blöcken wird über Nachrichtenqueues 
und Priorisierung vom Framework sichergestellt. Die Ausführung der Funktionsblöcke wird einstellbar periodisch eventgesteuert 
ausgelöst und beginnt meist mit dem Anstoß zum Auslesen der Sensoren, welche dann das Event über die Queues an andere Funktionsblöcke
weitergeben.
Von der Benutzeroberfläche aus lassen sich die Funktionsblöcke grafisch miteinander verbinden, welches die Kommunikation zwischen 
den Blöcken repräsentiert, und die Ausführung der Funktio-nen starten. Des Weiteren stellt die Oberfläche weitere fertige
Funktionsblöcke zur Verfügung, etwa zum Anzeigen und Abspeichern von Kamerabildern und Sensordaten oder zum  Abspielen selbiger.
Durch Referenzen aus dem Programm heraus, können über die Benutzeroberfläche be-stimmte Parameter im Code zur Initialisierung und 
zur Laufzeit gesetzt werden.


4 Implementierung
4.1 Umgebung
Die Zielsetzung sieht vor, das Auto auf eine blaue Linie verfolgen zu lassen. Deswegen ist mit Klebeband ein Oval auf den Boden
geklebt. Das verwendete Klebeband ist recht Matt, um Spiegelungen zu vermeiden. Außerdem ist das Fenster zugestellt, um die 
Sonneneinstrahlung zu minimieren.
TODO: Bild(er) der Umgebung

4.2 Struktur
Das Projekt ist in verschiedene Aufgabenbereiche aufgeteilt, die separat bearbeitet werden. 

TODO: Bild der angepassten Roasmap
4.2.1 Linienverfolgung
Damit das Auto der blauen Fahrbahnlinie folgen kann, wird die Linie im Kamerabild detektiert. Anschließend wird die Position der 
Linie ausgewertet um daraus Steuersignale für die Lenkung zu gewinnen. Diese Aufgaben werden von zwei Filtern gelöst:

4.2.1.1 blueImgFilter
Dieser Filter markiert die Fahrbahnlinie und greift dafür auf die OpenCv Bibliothek zurück. Weil die Kamera hauptsächlich den 
(hellgrauen) Boden unmittelbar vor dem Auto sieht, kann man da-von ausgehen, dass die Fahrbahnlinie im relevanten Suchbereich der
einzig blaue Bildinhalt ist.
Zunächst wird das Eingangsbild mit OpenCv vom RGB- in den HSV-Farbraum transformiert, um die Festlegung von Ober- und Untergrenzen
für Blauwerte zu vereinfachen. Mittels der OpenCv-Funktion inRange werden anschließend alle nicht blauen Bildinhalte herausgefiltert 
und ein Binär-bild zurückgegeben, welches alle blauen Flächen markiert. Die endgültigen Farbwertgrenzen erge-ben sich durch Testläufe 
unter Realbedingungen.    

TODO: Abbildung 1 und 2 (Übernehme ich aus deiner Doku Julian)

4.2.1.2 OneLineDetect
Dieser Filter erhält das Binärbild und ermittelt die Position der blauen Linie relativ zur Position des Autos. Dafür wird in einer 
festgelegten Bildzeile nahe des Unteren Bildrandes nach der größ-ten Anzahl weißer, zusammenhängender Pixel gesucht. Durch die
Aufbereitung des Bildes durch den blueImgFilter kann davon ausgegangen werden, dass diese Pixel zur blauen Linie gehören. 
Anschließend wird die Mitte des weißen Pixelabschnittes berechnet und deren Abstand zur Bild-mitte ausgegeben. Dieser Abstand 
wird auf einen Bereich zwischen -100 und 100 normiert. Über die Filterproperties lässt sich eine Mindestbreite für die Anzahl 
weißer Pixel festlegen um die De-tektion noch robuster zu gestalten. Sind im aktuell zu bearbeitenden Bild keine oder zu wenig
wei-ße Pixel am Stück, so wird statt einer Zahl zwischen -100 und 100 der Wert -101 ausgegeben.
Für Debugzwecke wird die detektierte Linienmitte mit einem roten Kreis markiert.
Dieser Ansatz setzt voraus, dass das Kameraobjektiv auf der Mittelachse des Modelautos montiert ist.

TODO: Abbildung 3 (übernehme ich genauso)

4.2.2 Abstandserkennung
Die Abstandserskennung ist eine der wichtigsten grundlegenden Anforderungen an einen autonomen Roboter zur Ermöglichung von 
gefahrloser Navigation. In diesem Projekt wird sie auf Basis von Messwerten der zehn im Auto verbauten Ultraschallsensoren 
[Referenz auf USC-Modell-Bild aus Hardware-Doku-Part] implementiert.
Die Übersetzung der Mess- in Geschwindigkeitswerte wurde anhand einer linearen Funktion realisiert, die allerdings einen minimalen
Input-Grenzwert zum Anstoßen der Bewegung und gleichsam einen Maximal-Geschwindigkeits-Wert fest eingebaut hat, der unter der 
vollen Kapazität des Motors liegt.
Die statisch festgelegten Mindestabstandswerte für die einzelnen Sensoren führen allerdings zu der unerwünschten Eigenschaft, 
dass situationsunabhängig die Gewichtung der Sensorwerte konstant bleibt, die vor allem für ein Geradeaus-Fahren mit gewisser 
Geschwindigkeit optimal ist. Das ist beispielsweise in Kurven problematisch, wo das Hauptaugenmerk nicht mehr geradeaus liegen 
sollte, da die Geschwindigkeit durch verbeifahrend frontal erkannte Hindernisse unnötig gedrosselt werden würde. Daher wurde eine 
dynamische Anpassung des relevanten Blickkegels realisiert. Dazu wird über den Parameter des aktuell eingeschlagenen Lenkwinkels 
jeder Messwert zunächst gemäß einer Gaußkurve gewichtet, bevor der geringste Abstandswert in eine Geschwindigkeit übersetzt wird.
Konkret sieht man in Abbildung [x] die fixen Ablesungspunkte der einzelnen Sensoren auf der x-Achse, welche den Sensor-Winkeln in 
Blickrichtung des Autos angelehnt wurden. Der entsprechende Punkt auf der Gauss-Kurve gibt den jeweiligen Gewichtungs-Faktor an, 
wobei ein größerer Wert eine größere Unempfindlichkeit bedeutet. Gemäß Lenkwinkel wird nun die Kurve auf der x-Achse verschoben,
wodurch die Gewichtung sich auf die entsprechend seitlich liegenden Sensoren verschiebt.

4.2.3 Motoransteuerung
Die Hauptaufgaben der Motorsteuerung sind es auf Flags, wie zum Beispiel dem Emergencybreak-Flag zu reagieren und die 
Motorsteuerungsdaten, welche sie von dem Controller bekommt, an die Arduinocommunication zu senden.

In diesem Baustein wird auf Flags reagiert, um auf zum Beispiel plötzlich auftauchene Hindernisse angemessen zu reagieren. 
Das wird erreicht, indem Flags direkt aus den Blöcken, welche Sensordaten verarbeiten, gesendet werden und so verschiedene 
Verarbeitungsschritte durchgegangen werden müssen. Die Emergency Break wird erst wieder aufgehoben, sobald ein entsprechendes 
Signal von den Sensoren kommt.

Außerdem beherrscht dieser Baustein als einziger das Protokoll, welches zur Ansteuerung der Motoren mittels der Arduinocommunication 
verwendet wird. So kann intern ein Protokoll verwendet werden, dass sich deutlich besser lesen und im Falle von Verarbeitungsfehlern 
debuggen lässt. Da der Motorcontroller eine gewisse Anzahl an Nullen benötigt, um aktiviert zu werden, werden die ersten 
150 Geschwindigkeitsdaten auf Null gesetzt. Nach diesen 150 Nullen können dann Geschwindigkeiten an die Motoren gesendet werden. 
Um eine Geschwindigkeit zu verhindern, die zu schnell für die Ultraschallsensoren ist, ist die maximale Geschwindigkeit auf 12% 
festgelegt. Sollte ein Wert, der größer als dieser ist, an die Motorsteuerung gesendet werden, so wird dieser ignoriert und ein 
Fehler in der Konsole ausgegeben.

4.2.4 Interprozesskommunikation
Da die Programmierung in verschiedenen Funktionsblöcken erfolgte, musste eine einheitliche Kommunikation sichergestellt werden.
Hierzu wurde eine separate Funktionssammlung erstellt, die von allen Funktionsblöcken inkludiert wurde. Sie beinhaltet Definitionen 
über den Nachrich-tentyp und die Zusammensetzung strukturierter Datentypen. Da der vom ADTF gestellte Ablauf zum Senden oder Empfangen 
eines bestimmten Nachrichtentyps zur Codeverdopplung in den Funktionsblöcken führen würde, wurden die Funktionen zusammengefasst und 
ausgelagert. Damit ließ sich auch einheitlich sicherstellen, dass Funktionsblöcke beim Empfangen die Nachricht erst Kopierten, bevor 
deren Inhalt verändert wurde.

4.3 Komplikation und Fehler
5 Auswertung


6 Literaturverzeichnis
[1] 	I2C Geschichte,  https://www.i2c-bus.org/twi-bus/
[2]	Das I²C-Bus-System
◦	http://www.g-heinrichs.de/pdv/i2c.pdf
◦	http://www.robot-electronics.co.uk/i2c-tutorial
◦	http://www.atmel.com/Images/8183S.pd

